# Πολυπαραμετρικές Μέθοδοι Ανάλυσης

## Εισαγωγή

Οι πολυπαραμετρικές στατιστικές μέθοδοι μας επιτρέπουν την μέγιστη αξιοποίηση των πληροφοριών που περιέχονται σε μια σειρά δεδομένων και ορισμένες από αυτές. Ταυτόχρονα μας επιτρέπουν να ελαχιστοποιήσουμε τον αριθμό των αντικειμένων που απαιτούνται για την εξαγωγή μοντέλου.

![Φασματα ως πολυπαραμετρικες συναρτησεις]()

Τα φάσματα είναι εν γένη πολυπαραμετρικές συναρτήσεις παρότι συχνά απεικονίζονται σε δύο μόνο διαστάσεις (συνήθως μορφής:
$S=f(x)$, όπου x μπορεί είναι κυμματάριθμος ppm κλπ). 
Τα δεδομένα μας είναι ουσιαστικά υπό μορφή πίνακα 2D όπου οι σειρές είναι δείγματα και οι στήλες είναι χημικά δεδομένα/παράμετροι. Συνήθως μετράμε πάνω από 6 χιλιάδες μεταβλητές (εννοούμε εδώ κάθε τιμή πχ συχνότητας) αλλά συχνά σημασία έχουν ~6. Είναι ιδιαίτερα δημοφιλείς στις διάφορες τεχνολογίες των -omics.
Οι μονοπαραμετρικές μεθοδολογίες είναι ακατάλληλες γενικά. Για παράδειγμα αν κάνω σε ζεύγοι t-test, θα έχω πιθανότητα σφάλματος πρώτου είδους:
$$
Risk=(0.05)^k
$$

Για στάθμη εμπιστοσύνης 95%.

## Κατηγοριοποίηση Πολυπαραμετρικών

Μπορούν να διακριθούν σε *γραμμικές* και *μη-γραμμικές*:

Γραμμικές|Μη-Γραμμικές
--------| ------
1|Neural Networks
2|Support Vector Machines

Για τις γραμμικές:

Τύπος|Πίνακας Δεδομένων|Προυποθέσεις
-----|-----|-----
Κλασσσικές|σδσ|σδσδα
???

Μπορούμε να τις διακρίνουμε ως προς το *σκοπό*:

Ταξινόμηση|Ανάλυση Σμηνών
--------|-----------
1|1
2|2

Βασικές Εφαρμωγές:

* Έλεγχος Ποιότητας
* Βελτιστοποίηση Ποιότητας
* Βελτιστοποίηση και Έλεγχος
* Ανάπτυξη & Βελτιστοποίηση Μεθόδων Προσδιορισμού
* Ταξινόμηση βακτηρίων, ιών ιστών κλπ
* Metabolomics
* Ανάλυση Οικονομικών Μεγεθών
* Σχεδιασμός Νέων Φαρμάκων
* Ανάπτυξη Νέων Υλικών
* Προσδιορισμός Επικινδυνότητας στην Τοξικολογία (Risk Assessment)
  
Για παράδειγμα μπορεί να ζητήμε να προσδιορίσουμε κάποιο δευτερεύον συστατικό που ενισχύει τη δράση της κύριας δραστικής ουσίας
Μπορούμε να προσδιορίσουμε επακριβώς τον τύπο βακτηρίου σε αίμα αισθενούς (ακόμη και τον ακριβή ορότυπο)

![συχνά οι αποφάσεις είναι προφρανείς]()

Εδω εξαιτάζουμε τη δραστηκότητα τριών συστατικών ως προς την συγκέντρωσή του.

![πίνακας δεδομένων]()

## Principal Components Analysis

Συχνά έχουμε να διαχειρηστούμε ένα τεράστειο όγκο δεδομένων που είναι εξαιρετικά δύσκολο να τα διαχειρηστούμε. Θέλουμε επομένως κάποιο τρόπο να μειώσουμε το πλήθος ανεξάρτητων μεταβλητών και με τρόπο που να επιτρέπει να εξάγουμε αποτελεσματικά, γενικά συμπεράσματα. Ο σημαντικότερος τρόπος να το επιτύχουμε αυτό είναι η μαθηματική μέθοδος *Principal Components Analysis (PCA)*. Η PCA περιλαμβάνει ουσίαστηκα στην κατασκευή νέων, τεχνιτών μεταβλήτων (τα *Principal Components (PC's)*). Αυτές οι νέες, τεχνιτές μεταβλητές είναι γραμμικός συνδισμός των αρχικών και κατασκευάζονται με τέτοιο τρόπο ώστε να *μεγιστοποιούν την μεταβλητότητα*. Μπορούν ελένξουμε πόσο  PC's θα παράξουμε: $(PC_1,PC_2,PC_3,[\;\dots], PC_n)$, όπου θα πρέπει $n\le N$, δηλαδή μικτρότερο ή ίσο του αρχικού αριθμού ανεξάρτητων μεταβλητών. Οι μεταβητότητα ανά άξονα μειώνεται σταδιακά,  δηλαδή ο PC_1 θα "απορροφήσει" τη μεγαλύτερη μεταβλητότητα, PC_2 λίγο μικρότερη κλπ. Τα $PC_i$ έχουν ακόμη μία ιδιότητα, είναι *αμηβαίως ανεξάρτητα* δηλαδή μαθηματικώς είναι *ορθογώνια μεταξύ τους* / κάθετα.
Αυτή η ιδιότητα μα επιτρέπει να πετύχουμε δύο πράγματα:

* **Μειώση διαστάσεων** (λιγότερες ανεξάρτητες μεταβλητές)
* **Απεικόνηση** (είναι και τρόπος απεικόνσης των δεδομένων)
  
Η λογική πίσω από την PCΑ είναι διερευνούμε τους παράγωντες που είναι σημαντικότεροι γιατί, με την έννοια ότι προκαλούν τις *μεγαλύτερες διακυμάνσεις* κάποιου μεγέθους που μας ενδιαφέρει (πχ Σήμα, Υγεία, Προγνωστικά Εξέλιξης κάποιας πάθησης κοκ)

![PCA charts]()

Αυτά τα διαγράμματα ονομάζεται *Scores Plot*. Στα πλαίσια της απεικόνησης μπορούμε να ψάξουμε για μοτίβα, όπως για παράδειγμα για ομάδες τιμών (*clusters*). Οι "ιδιότητες" των παρατηρήσεων ονομάζονται και *loadings* (πχ μεταβολίτες σε ασθενείς, συστατικά μήλων, κλπ).
Τα PCA's ονομάζονται και *λανθάνουσες μεταβλητές (latent variables)* γιατί ως γραμμικοί συνδιασμοί πολλαπλών μεταβλητών είναι δύσκολο να παρατηρηθούν στα δεδομένα ως έχουν. Από μια διαφορετική οπτική η PCA στοχεύει στη μέγιστοποίηση του διαχωρισμού 

$$

t_1=
\begin{bmatrix}x_1&x_2&x_3&x_4
\end{bmatrix}
\begin{bmatrix}
p_1\\p_2\\p_3\\p_4
\end{bmatrix}
 = \sum{p_ix_i}

$$

Η διαδικασία απλοϊκά έχει ως εξής:

1. Για το σμήνος υπολογίζων το κεντροειδές, δηλαδή το σημείο με συντεταγμένες τις μέσες τιμές των σημείων $C(\bar{x},\bar{y},\bar{z})$
2. Τα σημεία μετατοπίζονται όλα στο κεντροειδές (με κατάλληλες προσθαφαιρέσεις)
3. Κάνουμε κλιμάκωση *scaling*, διότι οι διάφορες μεταβλητές μπορεί να κειμένονται μεταξύ πολύ διαφορετικών ορίων, οπότε θα έχουν και πολύ διαφορετική επίδραση στα στην μεταβλητότητας. Ουσιαστικά, δηλαδή κάνουμε *κανονικοποίηση*. Για να πετύχουμε αυτή τη κανονικοποίηση μεταξύ των διαφορετικών ευρών τιμών, διαιρούμε κάθε τιμή μίας μεταβλητής με την μέση τιμή. Τότε οι τιμές θα κυμαίνονται μεταξύ $(\forall i\in \mathbb{N})\;\;x_i\in[0,1]$
4. Χαράσσεται η βέλτιστη ευθεία
5. Πάνω στον νέο άξονα προβάλλονται όλες οι τιμές
6. Ο νέος άξονας αυτός είναι το πρώτο $PC_1$ η πρώτη κύρια συνιστώσα.
7. Μπορούμε να χαράξουμε άλλες συνιστώσες έτσι ώστε αυτές να είναι όλες ορθογώνιες, με την ίδια πορεία που είδαμε άνω

Σταδιακά ορίζουμε:

1. **Μια ευθεία**
2. **Ένα επίπεδο**
3. **Μια σφαίρα**
4. **Υπερσφαίρες**

>**Παρατήρηση:**\
Κάθε νέα μεταβλητή εμπεριέχει πληροφορίες από *όλες* τις αρχικές μεταβλητές (γιατί είναι γραμμικός συνδαισμός τους)

### Επιλογή PC's

Ένα βασικό πρόβλημα που μας απασχολεί είναι το *πόσες κύριες συνιστώσες θα κρατήσουμε*. Είναι εμπειρική διαδικάσία. Όπως σημβαίνει σε κάθε μοντέλο μηχανικής μάθησης
πάρα πολλά PC's οδηγεί σε *υπερπροσαρμωγή (overfitting)*.

>**Ορισμός:**\
Γενικά *overfitting* ονομάζουμε το φαινόμενο όπου κάποιο μοντέλο έχει σημαντικά μεγαλύτερη απόδωση στη δεδομένα εκπαίδευσης απ' ότι στα πραγματικά.

>**Παρατήρηση:**\
Ουσιαστικά overfitting συμβαίνει όταν το μοντέλο μας προβλέπει πολύ κατά τα δεδομένα που του δώσαμε αλλά *δεν γενικεύει αποτελεσματικά*, δηλαδή σε μεταγενέστερα, δεδομένα, έχει σημαντικά χειρότερη απόδωση.

### Διασταυρούμενη Επικύρωση-Cross-Validation

Κάθε τέτοιο μοντέλο (εκτός Bayes) θα πρέπει να αξιολογηθεί ως προς την αποτελεσματικότητα του. Γι αυτό το σκοπό, από τα αρχικά δεδομένα μας κρατάμε το $^1/_7$ τυχαια και εκπαιδεύουμε το μοντέλο με τα υπόλοιπα. Στη συναίχεια εξαιτάζουμε τις προβλέψεις του μοντέλου αφού προσαρμωστούν σε αυτό τα δεδομένα. Ορίζουμε δύο συντελεστές, ο $R^2$ δίνει τη αποτελσματικότητα της προσαρμωγηής των δεδομένων και ο $Q^2$ που δίνει την προβλεπτική αξία του μοντέλου (με δεδομένα στα οποία δεν εκτεθεί ακόμη, ή ισοδύναμα, με δεδομένα τα οποία δεν εχουμε προσαρμώσει σε αυτό).

![R,Q διάγραμμα]()

### Έκτροπες Τιμές - Καμπύλη Hotelling

Μπορούμε να ορίσουμε μια καμπύλη, την *καμπύλη Hotelling* που έχει μορφή έλλειψης που περικλύει τα δεδομένα. Κάθε τιμή εκτός του ορίου που ορίζει αυτή η έλλειψη θεωρείται *έκτροπη τιμή (outlier)*. Συμβολίζεται ($Τ^2$). Ποιοτικά, αυτή η καμπύλη ορίζει τη φυσιολογική ή αναμενόμενη περιοχή τιμών του διαγράμματος αντικειμένων (*score plot*).

>**Παρατήρηση:**\
Το *όριο εμπιστοσύνης (confidence interval)* εκφράζει τον αριθμό των τιμών που επιτρέπουμε να είναι λάθος. Αυστηρότερα, είναι η πιθανότητα ότι μία πληθυσμιακή παράμετρος θα βρεθεί μεταξύ κάποιο διάστημα τιμών για κάποιο κλάσμα του πληθυσμού. Είναι τρόπος εκτίμησης της αβεβαιότητας.\
Για παράδειγμα με το συχνό 95% όριο εμπιστοσύνης σε καθένα από 100 δείγματα, τότε περίππου 95 από αυτά τα δείγμα θα εμπεριέχουν την πραγματική μέση τιμή ($\mu$)

>**Παρατήρηση:**\
Η PCA, όταν χρησημοποιείται ως προβλεπτικό μοντέλο, είναι παράδειγμα μεθόδου χωρίς επίβλεψη (*unsupervised*)

### DModX

### PLS

### PLS-DA

*Supervised* παραλλαγή της PCA. Επιδέχεται Cross-Validation.

### NOMIS

### CCMN
