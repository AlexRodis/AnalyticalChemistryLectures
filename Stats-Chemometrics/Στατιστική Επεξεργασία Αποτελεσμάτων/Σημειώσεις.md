# Στατιστική Επεξεργασία Αποτελσμάτων

## Σφάλματα

Τα πειραματικά σφάλματα χωρίζονται σε διάφορες κατηγορίες. Οι δύο κυριότερες είναι *τυχαία* και *συστηματικά*.

>**Συστηματικά** ή καθορισμένα σφάλματα ονομάζονται αυτά που οφείλονται σε έναν ή περισσότερους λόγος προσδιορίσημης αν όχι γνωστής προέλευσης.

>**Τυχαία** ή  και ακαθόριστα ονομάζονται τα σφάλματα που οφείλονται σε ακαθόριστα αίτα και είναι ουσιαστικά στατιστηκός "θόρυβος" που πάντα υπάρχει σε κάποιο βαθμό.

>**Ολικό** σφάλμα είναι το άθροισμα όλων των επιμέρους σφαλμάτων δηλαδή στατιστικών και συστηματικών. Αναφέρεται και ως *μικτό*.

>**Μονοκατευθυνόμενα** είναι τα σφάλματα που εμφανίζουν συστηματικό πρόσυμο. Μπορεί και είναι μονοκατευθυνόμενο εντός μικρότερων υπο περιοχών. Είναι γενικά συστηματικά.

>**Απόλυτο** ονομάζεται η διαφορά της "πραγματικής" από την πειραματική τιμή. Μπορεί να εξαρτάται από τη συγκέντρωση ή όχι.

>**Σχετικό** ονομάζεται το σφάλμα αναγόμενο στο μέγεθος (απόλυτο ανά μέγεθως).

Μπορούμε να διερευνήσουμε τη σφάλματα με απλά διαγράμματα συσχέτησης πειραματικής τιμής έναντι πραγματικής. Γενικά μιά έλλειψη τυχαιότητας στο πρόσημο δηλώνει συστηματικό σφάλμα.

![correlation plots]()

Γενικά ή ύπαρξη *τάσης* (ή *ολίσθησης*) των τιμών (είτε ανοδικής είτε καθοδικής) δείχνει οτι η μητριτική διαδικάσία είναι εκτός ελέγχου και η μέση τιμή είναι στατιστικά άχρηστη. Αυστηρότερα προσδιορίζεται ως εξής. Λαμβάνουμε τις επαναλαμβανόμενες τιμές και προσαρμόζουμε ένα γραμμικό μοντέλο:

$$
Y=w_0N+w_1\\
$$

Τότε αν η κλίση είναι στατιστικά σημαντική $w_0\ne 0 \pm s_{w_0}$ τότε υπάρχει ολίσθηση. Μια απλή περιγραφή των σφαλμάτων είναι:

$$
x_i=\mu+ b+ e
$$
Για μία μέτρηση. Όπου $b$ είναι το συστηματικό και $e$ το τυχαίο σφάλμα. Αντίστοιχα, για πολλαπλές μετρήσεις:

$$
x_i=\mu+(\bar{x}-\mu)+(x_i-\mu)
$$

Μπορούμε να αποικονήσουμε τις τιμές ως μια *κατανομή* σε ένα *ιστόγραμμα*, δηλαδή μια απεικόνηση ευρών τιμών (bin) για συνεχείς μεταβλητές ή μεμονομένων τιμών για διακριτές μεταβλητές. Ο μέγιστος επιθυμητός αριθμός *κλάσσεων* ή εύρος ανά κλάσση δίνεται από τον *κανόνα Sturges*

>**Κανόνας Sturges:** $k = 1+3.322\log_{10}{N}$

Στη πράξη, ο αριθμός προσαρμώζεται πάνω ή κάτω ώστε να είναι ποιό στρογγυλά τα εύροι. Μια εναλλακτική παρουσίαση της κατανομής είναι μέσω της *συσσωρευτικής συνάρτησης κατανομής*. Σε αυτή τη περίπτωση σε κάθε κλάση προστίθενται οι παρατηρίσεις των προηγούμενων.

Αριθμητική περιγραφή των πληθυσμών είναι οι παράμετροι *θέσης* ,*διασποράς* και *συμμετρίας*.

## Παράμετροι Θέσης

Οι παράμετροι θέσης πληθυσμού είναι:

* Μέση τιμή $\bar x =\frac{\sum{x_i}}{N}\;\;\; \bar x \rightarrow \mu, n \rightarrow \infty $
* Διάμεση Τιμή $x_{ \frac {n+1}{2} } n:\;\;\;περιττός\;\;\; \frac {x_{ \frac{n}{2} } + x_{\frac {n}{2}} +1 }{2}$
* Ρυθμισμένη Μέση Τιμή
* Επικρατούσα Τιμή
  
## Παράμετροι Διασποράς

Οι βασικές παράμετροι διασποράς είναι:

* Εύρος: $R=x_n-x_1$
* Τυπική Απόκλιση: $s=\sqrt{\frac {\sum_{i=1}^N{(\bar {x}-x_1)^2 }}{N-1} }$, ισχύει ότι $s \rightarrow\sigma, n\rightarrow\infty$
* Σχετική Τυπική Απόκλιση: $s_t=^{s}/_{\bar{x} }$
* Εκατοστιαία Τυπική Απόκλιση: $\%RSD \equiv\%CV=\frac {s}{\bar {x}}*100\%$
* Διακύμανση: $\nu\equiv Var\;\;\; s^2, \sigma^2$
* Μέση Απόλυτη Απόκλιση: $amd=\frac {\sum_{i=1}^N{|{\bar{x}-x_i|}}}{N}$
* Τυπικό Σφάλμα: $SE=\sqrt{\frac {\sum_{i=1}^N{(x_i-\bar{x})}}{N(N-1)}}$
* Εύρος Μεταξύ Ποσοστημοριακών Σημείων
  
## Παράμετροι Συμμετρίας

Οι παράμετροι συμμετρίας είναι μόνο δύο και μετράνε ουσιαστικά το ίδιο πράγμα:

* Ασυμμετρία: $g_i=\frac{\sum_{i=1}^{N}{(x_1-\bar{x})^3}}{(N-1)(N-2)s^3}$
* Κύρτωση: $g_2=\frac 1n \sum_{i=1}^N[\frac {x_i-\bar{x}}{s}]^4-4$
  
## Κανονική Ή Gaussιανή Κατανομή

Η κατανομή Gauss είναι πανταχώς παρούσα στη φύση. Αυτό οφείλεται στο *Κεντρικό Οριακό Θεώρημα*. Θα δούμε μια απλοϊκή διατύπωση γιατί η μαθηματική απόδηξη του είναι εξαιρετικά πολύπλοκη.

>**Κεντρικό Οριακό Θεώρημα:**\
>*Το άθροισμα τιμών προερχόμενων από έναν ή περισσότερους πληθυσμούς με οποιαδήποτε αρχική κατανομή, τείνει να αποτελέσει στοιχείο πληθυσμού με κανονική κατανομή,όσο αυξήνει ο αριθμός των αθροιζόμενων τιμών.*

Δηλαδή, απλοίκά μιλώντας αν $Α$ κάποια αυθέρετη κατανομή τότε:
$$
Α\rightarrow N(\sigma,\mu)
$$

Όπως είδαμε μπορούμε να επικονήσουμε την *κατανομή* ενός πληθυσμού, με ένα ιστόγραμμα είτε χρησιμοποιώντας την συχνότητα είτε την cdf (*cummulative density function*). Θέλουμε από μια κατανομή που αντιπροσωπεύει ένα συγκεκριμένο πλυθησμό, να καταλήξουμε σε κάποιον "παγκόσμιο" τρόπο κατανομής.

>**Παρατήρηση:**\
>Υπάρχουν γενικά, διάφορες κατανομές, όπως Διωνυμική, Bernoulli κλπ. Συνήθως μια τέτοια κατανομή είναι μια γενική περιγραφή ενός *τύπου* κατανομής. Μια συγκεκριμένη μορφή της κάθε κατανομής προσδιορίζεται από (i) την γενική μορφή στην οποία συμμοφώνεται (πχ Διωνυμική,Gauss), και (ii) ένα σύνολο παραμέτρων που δίνουν την ακριβή μορφή. Αυτές οι παράμετροι διαφέρουν για κάθε τύπο κατανομής. Για την Guass οι δύο αυτές παράμετροι είναι η τυπική απόκλιση (*σ*) και το μέσο (*μ*). Η Διωνυμική έχει ένα παράγωντα που ονομάζεται (*p*). Έτσι θα συμβολίζουμε γενικά
>$$
Χ(e_1,e_2,[\dots], e_n)
$$
>και θα εννούμε μια κατανομή γενικού τύπου ($X$) με παραμέτρους $e_1,e_2,\dots, e_n$. Με αυτή τη σύμβαση, θα γράφουμε
>$$
N(\mu,\sigma)
$$
>και θα εννοούμε κατανομή Gauss με μέση τιμή $\mu$ και τυπική απόκλιση $\sigma$.

Η κατανομή Gauss δίνεται από την συνάρτηση κατνανομής $f$ που δίνει τη συχνότητα συναρτήση της παραμέτρου $χ$ και έχει ως εξής:

$$
f(x) = \frac {1}{\sigma \sqrt{2\pi} }e^{=\frac {(x-\mu)^2}{2{\sigma}^2} }
$$

![κατανομή Gauss βασική]()

Μια άλλη ενδιαφέρουσα μορφή της συνάρτησης προκύπτει θέτοντας απλά $z=\frac {x-\mu}{\sigma}$. Τότε η συνάρτηση γίνεται:

$$
f(x) = \frac{1}{\sqrt{2\pi}}\int_{t=-z}^{t=+z}{e^{-t^2}dt}
$$
Αυτή η απεικόνηση δίνετ τη συχνότητα συναρτίση του αριθμού τυπικών αποκλίσεων. Οι τιμές του ολοκληρώματος μπορούν να προκύψουν από πίνακες ή με αριθμητικές μεθόδους (*βλ. SciPy.special.erf,  SciPY.stats.multivariate_normal,  SciPy.stats.norm*).

![μηχανη πληθυσμου Gauss]()

### Το Νόημα των Κρίσιμων Τιμών

Έστω $X(x_1,x_2,x_3,[\dots],x_N)$ με ${x_1,x_2,x_3,[\dots],x_N}$ ν τυχαία επιλεγμένα στοιχεία κανονικού πληθυσμού $Χ$. Για κάθε Ν-άδα υπολογίζεται μια τιμή στατιστικού στοιχείου πληθυσμού $R$ από πληθυσμό $**R**$. Όλοι οι "σχηματισμοί" των τιμών χ προέρχονται από τον ίδιο πληθυσμό και επομένως ισχύει γι αυτούς η μηδενική υπόθεση. Συμβαίνει κάποιοι "ακραίοι" σχηματισμοί Ν στοιχείων να οδηγούν σε "ακραίες" τιμές στατιστικού στοιχείου. Γι αυτούς η μηδενική υπόθεση απορρίπτεται λανθασμένα.

## Κατανομή Μέσω Τιμών

$$
\sigma_{\bar{x}} = \frac {\sigma_x}{\sqrt{N}}
$$

## Στατιστικές Δοκιμασίες Σημαντικότητας

Στις δοκιμασίες σημαντικότητας γνεικά, ξεκινάμε με μια αρχική υπόθεση $Η_0$ που ονομάζεται *μηδενική υπόθεση*, και εκτελούμε μια στατιστική δοκιμασία. Η δοκιμασία θα μας οδηγήσει στην αποδοχή της μηδενικής υπόθεσης ή την αποδοχή της, μέ κάποι δεδομένα όρια μέγιστης πιθανότητας λάθους (*στάθμη εμπιστοσύνης*). Όταν απορρίπτουμε την μηδενική υπόθεση, υιοθετούμε την *εναλλακτική υπόθεση* $H_a$. Η ταυτότητα των $H_0$ και $H_a$ εξαρτάται από την δοκιμασία. Γενικά ως μηδενική υπόθεση δεχόμασται οτι δεν υπάρχει σημαντική διαφορά και ως εναλλακτική οτι υπάρχει.
Μερικές κοινές εκδοχές είναι:

>$H_0$ οι διαφορές των δύο είναι τυχαίες, οπότε $H_a$ οι διαφορές δεν είναι τυχαίες\
>$H_0$ η τιμή ανοίκη στον πληθυσμό, $H_a$ η τιμή δεν ανοίκει στον πληθυσμό\
>$H_0:$ $\bar{x}= \mu$, $H_a: \bar{x}\ne\mu$\
>$H_0:\mu_A = \mu_B\;\; H_A: \mu_A\ne\mu_B$\
>$H_0:\sigma_A=\sigma_B,\;\;H_A:\sigma_A\ne\sigma_B$\
>$H_0:X_i\in A_i\;\; H_a:X_i\notin A_i$


>**Παρατήρηση:**
>Ισχύει πάντα:
>$$
P(H_0)+P(H_a) = 1
>$$

Σφάλμα **πρώτου είδους** έχουμε όταν απορρίπτουμε την μηδενική ενώ ισχύει και σφάλμα **δεύτερου είδους** έχουμε όταν απορρίπτουμε την εναλλάκτική υπόθεση ενώ αυτή ισχύει. Η *στάθμη εμπιστοσύνης* εκφράζει την ελάχιστη πιθανότητα σφάλματος πρώτου είδους. Είναι συνήθως 95%, σπανιότερα 90% (όταν οι απαιτήσεις είναι χαμηλές) ή 95% (όταν οι απαιτήσεις είναι υψηλές).

![σφάλματα και στάθμες εμπιστοσύνης]()

Το αποτέλεσμα μια δοκιμασίας γενικά αποδίδεται μέσω της τιμής $P$ που είναι η πιθανότητα σφάλματος πρώτου είδους. Όταν αυτή είναι **μεγαλύτερη** από την τιμή 1-στάθμη εμπιστοσύνης τότε η μηδενική ισχύει. Έστων $CL$ ή στάθμη εμπιστοσύνης. Τότε:

$$
P>1-CL \implies H_0\\
P<1-CL \implies H_a
$$

Διότι η τιμή $1-CL$ είναι η πιθανότητα σφάλματος πρώτου είδους. Για παράδειγμα αν $P=0.03\;\; και\;\; CL=90\%$

## Δοκιμασία T ή Student's Test ή T-Test

Μια από τις ποιό διαδεδομένες δοκιμασίες είναι η δοκιμασία Τ, η οποία έχει 2-3 παραλλαγές. Κατ' αρχάς η δοκιμασία βασίζεται στην εξής διαδικασία. Από μια κανονική κατανομή επιλέγονται Ν στοιχεία τυχαία, και τροφοδοτούνται σε μία συνάρτηση που δίνει μια τιμή τ. Αυτή η τιμή ανοίκει σε έναν νέο πληθυσμό (Τ). Η αρχική κανονική κατανομή απόδίδει τη διασπορά των πληθυσμιακών τυπικών αποκλίσεων ως προς την κεντρική τιμή (μ). Η παράγωγη κατανομή Τ αποδίδει τη διασπορά των τιμών ως προς την κεντρική κατανομή. Οι δε τιμές t είναι ο αριθμός των *δειγματικών τυπικών αποκλίσεων,s*. Η κατανομή Τ εμφανίζει ιδαίτερα έντονη διασπορά σε μικρές τιμές Ν. Υπάρχει έντονη αμφιβολία ως προς το κατά πόσο η τιμή s είναι πραγμτικά αξιόπιστη εκτιμήτρια της σ. Όσο η τιμή Ν αυξάνει, τόσο η κατανομή τείνει στην κανονική. Ο τύπος που χρησιμοποιούμε εξαρτάται από την ακριβής δοκιμασία:
$$
t_{exp} = \frac {|\mu -\bar{x}|\sqrt{N}}{s}\;\;\;Σύγκριση\; πραγματικής\; τιμής\; και\; μέσης\\
t_{exp}=\frac {|\bar {x_A} - \bar{x_B}|}{s_{A,B}\sqrt {\frac {1}{N_A}+{\frac {1}{N_B}}}}\;\; s_{A,B}=\frac {\sum_{i=1}^{N_A}{(\bar{x_A}-x_{i,A})^2} +\sum_{i=1}{N_B}{(\bar {x_B} - x_{i,B})^2} }{N_A + N_B-1}\;\; Σύγκριση\; δύο\; πειραματικών\; μέσω\; τιμών\\
t_{exp}=\frac {|\bar{d}|\sqrt{N}}{s_d}\;Δοκιμασία\; πειραματικών\; τιμών\; κατά\; ζεύγοι

$$

### Δοκιμασίες Ενός ή Δύο Άκρων

Τα Τ-Τεστ διακρίνονται σε δύο κατηγορίες ανάλογα με την *εναλλακτική υπόθεση*. Στη δοκιμασία δύο άκρων η εναλλακτική υπόθεση είναι οτι τα δύο μεγέθοι *δεν είναι διαφορετικά ή άνισα*. Στην δοκιμασία *ενός άκρου* η εναλλακτική υπόθεση είναι οτι το ένα είναι μεγαλύτερο (ή μικρότερο) από το άλλο. Δηλαδή:

$$
\begin{alignedat}{}
H_0: \; \bar {x_A}= \bar {x_B}\\
H_a :\;\bar{x_A}\ne \bar{x_B} \;Δύο \;άκρων\\
H_a: \; |\bar{x_A}-\bar{x_B}|\ne0\;δοκιμασία\; ενός\; άκρου
\end{alignedat}
$$

Ουσιαστικά, 

### Εκτέλεση T-Test
Για κάθε απλό t-Test ακολουθούμε τα εξής βήματα:

1. Αποφασίζουμε ποιόν τύπο t-test θα χρησιμοποιήσουμε, ανάλογα με το σκοπό μας. Για παράδειγμα, αν θέλουμε να συγκρίνουμε αν το αποτέλεσμα της μεθόδου μας συμφωνεί με το αποδεκτό ως πραγματικό (πχ αυτό μας μεθόδου αναφοράς ή του *πρότυπου υλικού αναφοράς*) θα εκτελέσουμε το πρώτο. Με βάση αυτό το κριτήριο επιλέγουμε τη σύναρτηση που τεστ.
2. Αποφασίζουμε για τη επιθυμητή *στάθμη εμπιστοσύνης*. Συνήθως είναι αυστηρά προκαθορισμένη. Για τις περισσότερες αναλυτικές εφαρμωγές είναι $CL=95\%$, δηλαδή ζητάμε πιθανότητα σφάλματο πρώτου είδους *το πολύ* 5%. Για κάποιες φαρμακευτικές εφαρμωγές, με αυστηρές απαιτήσεις είναι $CL=99\%$, δηλαδή μέγιστη πιθανότητα σφάλματος πρώτου είδους 1%.
3. Αποφασίζουμε αν το test μας είναι ενός ή δύο άκρων. Αυτό θα εξαρτηθεί από τη από την πληροφορία που ζητάμε από το αποτέλεσμα.
4. Βάση του πλήθους τιμών, καθορίζουμε τους *βαθμούς ελευθερίας* ($\nu$), ως εξής $\nu=\Nu-1$. 
5. Με βάση τη στάθμη εμπιστοσύνης , τον τύπου το t-test (τα άκρα) και τους βαθμούς ελευθερίας καθορίζουμε τη θεωρητική τιμή $t_{theoretical}$.
6. Βάση του τύπου του test εφαρμώζουμε τον κατάλληλο τύπο (άνω) και προκύπτει το $t_{experimental}$.
7. Αν $t_{experimental}>t_{theoretical}$ τότε η μηδενική υπόθεση απορρίπτεται και ισχύει η $H_a$ διαφόρετικά η υπόθεση διατηρρείται, δηλαδή $H_0$. Η απόλυτη διαφορά $|t_{experimental}-t_{theoretical}|$ δίνει μια 'νοερή' εκτίμηση για το πόσο κοντά είμαστε στην απόρριψη ή τη διατήρηση της $H_0$.
8. Στην περίπτωση της αναλυτική χημείας $t_{experimental}>t_{theoretical}$ συνήθως σημαίνει οτι η πειραματική τιμή δεν έχει σημαντική διαφορά από την ονομαστική (που είναι και το επιθυμητό).
9. Στη πραγματικότητα δεν συγκρίνουμε πλέον τα t (αυτά αναφέρονται σε πίνακες συνήθως). Τα πραγράμματα συνήθως δίνουν απλά την κατά πολύ ακριβέστερη τιμή $P$. Τότε αν $P>CL-1$ η μηδενική υπόθεση διατηρείται δηλαδή δεν υπάρχει στατιστικώς σημαντική διαφορά, μεταξύ ονομαστικής και πειραματικής τιμής, που είναι συνήθως το επιθυμητό.

>**Παρατήρηση:**\
Γενικότερα, *όσο μεγαλύτερη είναι η τιμή P τόσο βεβαιότεροι είμαστε οτι ισχύει η μηδενική υπόθεση $H_0$*


## Έκτροπες Τιμές (Outliers)

Ως *έκτροπη τιμή* σε ένα πληθυσμιακό δείγμα, χαρακτιρίζεται κάθε τιμή που ανοίκη σε δαιφορετικό πληθυσμό από αυτό της πλειονότητας των μετρήσεων.
Αυτές οι τιμές θα πρέπει να απορρίπτονται από τα δεδομένα μας, διαφορετικά επιρρεάζουν αρνητικά τα αποτελέσματα.
Χρειαζόμαστε ένα τυπικό τρόπο για να αναγνωρίζουμε και να απορρίπτουμε αυτές τις τιμές. Ο βασικότερος στην Αναλυτική Χημεία είναι η *δοκιμασία Q* (ή κρητίριο Q). 

![έκτροπες τιμές]()

Έστω μια σειρά μετρήσεων με σαφή σειρά (δηλαδή *διατεταγμένες*):
$$
x_1>x_2>x_3>[\;\dots]>x_n
$$
Πρέπει να ανοίκουν στο ίδιο πληθυσμιακό δείγμα. Βασίζεται στο διαχωρισμού του της άνω σειράς σε "υποπεριοχές". Εξαιτάζει την κατανομή των λόγων αυτών των περιοχών. Αυτό το κριτήριο μπορεί να απορρίψει **ακριβώς** μία έκτροπη τιμή και δεν μπορεί να επαναληφθεί. Επειδή οι τιμές είναι διατεταγμένες, η ύποπτη έκτροπη τιμή θα είναι η μικρότερη ($x_1$) ή η μεγαλύτερη ($x_n$). Τότε, ανάλογα με τη περίπτωση του t-test προκύπτει ένα $Q_{experimental}$ το οποίο συγκρίνεται με ένα $Q_{theoretical}$

$$
Q_{experimental}=\frac {x_2-x_1}{x_n-x_1}\;\;ύποπτη\; τιμή\; η\; x_1\\
Q_{experimental}=\frac {x_n-x_{_{n-1}}}{x_n-x_1}\;ύποπτη\; τιμή\; η\; x_n
$$

Η τιμή $Q_{theoretical}$ μπορεί να προκύψει από πίνακες (όπως και στο ttest). Υπολογιστηκά μπορεί να προκύψει ακριβέστερα με την χρήση *προσομοίωσης Monte Carlo*.

### Προσομοίωση Monte Carlo

Με τον ίδιο τρόπο προκύπτουν τα θεωρικά μεγέθοι για κάθε στατιστική δοκιμασία. Πολύ συνοπτικά ο αλγόριθμος έχει ως εξής:

1. Από έναν κανονικό πληθυσμό λαμβάνονται $Ν$ στοιχεία (συνήθως $N\in[3,12]$ ), τυχαία $\{\;x_1,\;x_2,\;x_3,[\;\dots]\;,x_n\}$
2. Υπολογίζεται το $Q_{experimental}$ του πλέον απόμακρου
3. Επαναλαμβάνουμε μέχρι να συλλέξουμε $M\in[10000-30000]\; Q_{experimental}$. 
4. Κατατάσσονται κατά αυξανόμενη τιμή
5. Οι τιμή υπ' αριθμόν $CL*M$ είναι το $Q_{theoretical}(N)$.
6. Τα Μ εδώ είναι *indices*

### Φαινόμενα Κάλυψης (masking)

Αν υπάρχουν πολλαπλές έκτροπες τιμές, τότε η παρουσία μια δεύτερης έκτροπης μπορεί να 'καλύψει' μια έκτροπη. Η σύγχρονη ανίχνευση πολλαπλών έκτροπων είναι γενικά δύσκολη. Υπάρχει δοκιμασία για δύο έκτροπες που ονομάζεται QP-test.

$$
\def\arraystretch{2}
\begin{array}{cc}
QP=\frac {(x_3-x_1)(x_3-x_2)}{(x_n-x_1)(x_n-x_2)}\\
QP=\frac {(x_n-x_{n-2})(x_{n-1}-x_{n-2})}{(x_n-x_1)(x_{n-1}-x_2)}\\
QP=\frac {(x_2-x_1)(x_n-x_{n-1})}{(x_{n-1}-x_1)(x_n-x_2)}
\end{array}
$$

![έκτροπες τιμές και τιμή P]()

Λόγο φαινομένων κάλυψης πρέπει πρώτα να ελέγχεται η πιθανή παρουσία **δύο** έκτροπων τιμών και μετά μίας (αλλιώς η δεύτερη θα "κρύψει την πρώτη").

## Δοκιμασία F, Snedecor's F-Test

Μια άλλη σημαντική δοκιμασία είναι η δοκιμασία F, ή F-Test. Αυτή η δοκιμασία απαντά στο ερώτημα, "οι διακυμάνσεις δύο σειρών τιμών, είναι διαφορετικές ή ίδιες (*στατιστικά σημαντικές*)?". Το στατιστικό στοιχείο εδώ (συμβολίζεται $F$):

$$
F = \frac {s_A^2}{s_B^2}\\
s_a>s_b \implies F_{experimental}\ge1
$$

Είναι εξαιρετικά σημαντική ως προκατεργασία για άλλες δοκιμασίες. Για παράδειγμα για κάνουμε ορθό T-Test μεταξύ δύο πειραματικών μέσων τιμών θα πρέπει να γίνει F-Test. Το test είναι διαφορετικό ανάλογα με το αν οι διακυμάνσεις είναι στατιστικά συμαντικές ή όχι. Η κρίσημη τιμή εδώ μπορεί να προκύψει από πίνακες ως συνάρτηση βαθμών ελευθερίες των δύο σειρών μετρήσεων ($\nu_A,\nu_B$) και τη στάθμη εμπιστοσύνης *της δοκιμασίας* (CL). Η δοκιμασία F μπορεί να είναι ενός (*"είναι η μία διακύμανση μεγαλύτερη από την άλλη"*) άκρου ή δύο άκρων (*"Είναι οι διακυμάνσεις διαφορετικές/άνισες?"*). Είναι ανάλογα με τα άκρα του T-Test.

## Ανάλυση Διακύμανσης (ANOVA)

Η ανάλυση διακύμανσης εντοπίζει τη προέλευση της διακύμασης ομαδοποιημένων μετρήσεων και ετιμά την συνεισφορά κάθε πηγές στην ολική διακύμανση.

Έστω οτι όλες οι μετρήσεις ενδιαφέροντως προέρχονται από τον ίδιο πληθυσμό. Τότε η παρατηρούμενη διακύμανση μεταξύ των μέσων τιμών των επιμέρους ομάδων, είναι στατιστικώς αναμενόμενη?. Αν είναι η αναμενόμενη τότε πράγματι όλες προέρχονται από ίδιο πληθυσμό. Αν όχι, τότε κάποιος παράγοντας τις διαφοροποιεί, αυξάνει τις διακυμάνεις και μετατρέπει έναν πληθυσμό σε πληθυσμούς.

Τυπικές εφαρμωγές είναι:

* Σύγκριση αποτελεσμάτων πολλών εργαστηρίων που μετράνει το ίδιο πράγμα(t-test για πολλαπλές ομάδες)
* Διαπίστωση της ομοιγένειας του δείγματος με αναλύσεις διαφορετικών επιμέρους δειγμάτων από διαφορετικά μέροι του αρχικού δείγματος
* Διαπίστωση σειράς επίδραση σειράς μέτρησης στο αποτέλεσμα
* Επίδραση ημέρας μέτρησης στο αποτέλεσμα

Πιθανές πηγές διακύμασης είναι τα *τυχαία σφάλματα* που υπεισέρχονται στο στάδιο τις μέτρησης και έχουν καθολική επίδραση, και *οι ελεγχόμενοι παράγωντες* που καθορίζουν το σύστημα ομαδοποίησης. Διακρίνονται σε αριθμήσημους και μη-αριθμήσημους. 
Υπάρχουν δύο βασικοί τύποι ANOVA, *η μονόδρομη* που εξαιτάζει έναν ελεγχόμενο παράγωντα και την *δίδρομη* που έχει δύο ελεγχόμενους παράγοντες.

Έστω $n$ σειρές $m$ μετρήσεων. Δηλαδή:
$$
\begin{array}{cc}
\;  & 1 & 2 &\dots &\dots &m\\
Μέτρηση\;1 &x_{11} & x_{12} &\dots &\dots &x_{1m}\\
Μέτρηση\; 2 & x_{21} & x_{22} &\dots &\dots &x_{2m}\\
Μέτρηση\; 3 & x_{31} & x_{32} &\dots &\dots &x_{3m}\\
Μέτρηση\; 4 & x_{41} & x_{42}&\dots &\dots &x_{4m}\\
\dots\\
Μέτρηση\; n & x_{n1} & x_{x2} &\dots &\dots &x_{nm}\\
\end{array}\\
H_0:\;\mu_1=\mu_2=\mu_3=\mu_4=[\;\dots]=\mu_m\\
$$

$$
H_a: 
\def\arraystretch{1.5}
\begin{array}{cc}
\mu_1\ne\mu_2=\mu_3=\mu_4=[\;\dots]=\mu_m\\
\mu_1=\mu_2\ne\mu_3=\mu_4=[\;\dots]=\mu_m\\
\mu_1=\mu_2=\mu_3\ne\mu_4=[\;\dots]=\mu_m\\
\dots
\end{array}
$$

Για κάθε ομάδα μετρήσεων:
$$
A_j = \sum_{i=1}^{n_j}{x_i}
$$

$$
\bar{x_j}=^{A_{j}}/{n_j}
$$
$$
Var_j=s_j^2=\frac {\sum_{i=1}^{n_j}{(\bar{x_j}-x_i)^2} } {n_j-1}
$$
$$
\sum_{i=1}^{n_j}{r_j^2} = \sum_{i=1}^{n_j}{(\bar{x_j}-x_i)^2} = Var_{_j}*(n_j-1)
$$
$$
N=\sum_{j=1}^{m}{n_j}
$$

$$
A=\sum_{j=1}^{N}{A_j}
$$

$$
S_1=\sum_{k=1}^{m}{\sum_{i=1}^{n_k}{r_i^2}}
$$
$$
S_2 =\sum_{k=1}^{m}{(\bar{\bar{x}}-\bar{x}_k)^2}
$$

$$
S_T=S_1+S_2
$$
$$
\def\arraystretch{2.5}
\begin{array}{cc}
S_1&f_1=N-m&Var_1=\frac{S_1}{f_1}\\
S_2&f_2=m-1&Var_2=\frac{S_2}{f_2}\\
S_T=S_1+S_2&f_T=N-1&Var_T=\frac{S_T}{f_T}
\end{array}
$$
Τ'ελος κάνουμε δοκιμασία F ενός άκρου για τις διακυμάνσεις $(Var_{_1},Var_{_2})$

### Ερμηνία ANOVA

Αν το F-test δώσει ίσες διακυμάνσεις τότε ο εξαιταζόμενος παράγωντας δεν επιδρά στο αποτέλεσμα. Όλες οι μετρήσεις προέρχονται από τον ίδιο πληθυσμό και οποιεσδήποτε διαφορές οφείλονται μόνο σε εγγενείς διακυμάνσεις. Όλες μπορούν να συνενωθούν σε μία ομάδα. Η μεγάλη μέση τιμή ($\bar{\bar x}$) είναι το τελικό, εννιαίο αποτέλεσμα της ανάλυσης. Η ολική διακύμανση ($V_{_Τ}$) είναι η διακύμαση του δείγματος και είναι εκτιμήτρια της διακύμασης του πληθυσμού. Η τυπική απόκλιση είναι $s_{_T}=V_{_T}^{1/2}$.

Αν είναι $V_1<V_2$ τότε: Ο εξαιταζόμενος παράγωντας *επιδρά* στο αποτέλεσμα, οι μετρήσεις προέρχονται από διαφορετικού πληθυσμούς και οι διαφορές των μέσων τιμών οφείλονται στον παράγωντα αυτόν. Δεν επιτρέπεται η συνένωση σε μια ενιαία ομάδα, η μεγάλη μέση τιμή δεν έχει σημασία, και δίνονται ξεχωριστά οι τιμές $V_1,V_2$ και οι αντίστοιχες τυπικές αποκλίσεις $S_T^1=v_1^{1/2} \;\;\; s_2=V_2^{1/2}$.

Ως παράδειγμα έστω οτι μετράμε κάποιο βαρέο μέταλλο και λαμβάνουμε δείγματα από 4 διαφορετικά βάθοι της θάλασσας και θέλουμε να μάθουμε αν το βάθος της δειγματοληψίας επιδρά στα αποτελέσματα της ανάλυσης (είναι ο ελεγχόμενος παράγωντας). Σε κάθε βάθος γίνονται πολλαπλές μετρήσεις και αυτές ομαδοποιούνται ανά βάθος. Υποθέτοντας στάθμη εμπιστοσύνης 95\% κάνουμε ANOVA (οι υπολογισμού είναι ως άνω και δεν έχουν νόημα με το χέρι). Έστω ότι βρίσκουμε ίσες διακυμάνσεις. Τότε θα συμπεράνουμε οτι το βάθος δειγματοληψίας (για το εξαιταζόμενο εύρος) δεν έχει σημασία. Αν απ' την άλλη βρίσκαμε $V_1<V_2$, τότε θα συμπαιρέναμε οτι το βάθος έχει σημαντική επίδραση στα αποτελέσματα και επομένως η δειγματοληψία θα πρέπει να γίνεται σε αυστηρά καθορισμένο βάθος (που θα το βρούμε στη βελτιστοποίηση της μεθόδου).
Ενδιαφέρον παρουσιάζει το οτι μπορούμε να πάμε ένα βήμα παρακάτω και να εξαιτάσουμε σε ποιό βάθος ξεκινά η σημαντική διαφοροποίηση. Τότε αν έστω μια σειρά μέσω τιμών των επιμέρους ομάδων (μέση τιμή για κάθε βάθος στο παράδειγμα), τότε:
$$
\def\arraystretch{3.5}
\begin{array}{cc}
Δείγμα  & Μέση\;\;Τιμή & \space& Διαφορά\\
A & \bar{x}_1&\space & d_1\\
B & \bar{x}_2&\space & d_2\\
C & \bar{x}_3&\space & d_3\\
D & \bar{x}_4&\space & d_4\\
E & \bar{x}_5&\space & d_5\\
&[\dots]
\end{array}
$$

$$
LSD = s_1*(\frac{2}{f_1})^{1/2}*t_{f_1}
$$

Όπου $t_{f_1}$ η κρίσημη τιμή t-test. Θα είναι $d_{i-1}<LSD<d_i$, οπότε η διαφοροποίση ξεκινά ανάμεσα στις αντίστοιχες τιμές της ελεγχόμενης παραμέτρου.