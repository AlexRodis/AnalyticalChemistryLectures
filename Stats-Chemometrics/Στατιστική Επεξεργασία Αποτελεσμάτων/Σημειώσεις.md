# Στατιστική Επεξεργασία Αποτελσμάτων

## Σφάλματα

Τα πειραματικά σφάλματα χωρίζονται σε διάφορες κατηγορίες. Οι δύο κυριότερες είναι *τυχαία* και *συστηματικά*.

>**Συστηματικά** ή καθορισμένα σφάλματα ονομάζονται αυτά που οφείλονται σε έναν ή περισσότερους λόγος προσδιορίσημης αν όχι γνωστής προέλευσης.

>**Τυχαία** ή  και ακαθόριστα ονομάζονται τα σφάλματα που οφείλονται σε ακαθόριστα αίτα και είναι ουσιαστικά στατιστηκός "θόρυβος" που πάντα υπάρχει σε κάποιο βαθμό.

>**Ολικό** σφάλμα είναι το άθροισμα όλων των επιμέρους σφαλμάτων δηλαδή στατιστικών και συστηματικών. Αναφέρεται και ως *μικτό*.

>**Μονοκατευθυνόμενα** είναι τα σφάλματα που εμφανίζουν συστηματικό πρόσυμο. Μπορεί και είναι μονοκατευθυνόμενο εντός μικρότερων υπο περιοχών. Είναι γενικά συστηματικά.

>>**Απόλυτο** ονομάζεται η διαφορά της "πραγματικής" από την πειραματική τιμή. Μπορεί να εξαρτάται από τη συγκέντρωση ή όχι.

>>**Σχετικό** ονομάζεται το σφάλμα αναγόμενο στο μέγεθος (απόλυτο ανά μέγεθως).

Μπορούμε να διερευνήσουμε τη σφάλματα με απλά διαγράμματα συσχέτησης πειραματικής τιμής έναντι πραγματικής. Γενικά μιά έλλειψη τυχαιότητας στο πρόσημο δηλώνει συστηματικό σφάλμα.

![correlation plots]()

Γενικά ή ύπαρξη *τάσης* (ή *ολίσθησης*) των τιμών (είτε ανοδικής είτε καθοδικής) δείχνει οτι η μητριτική διαδικάσία είναι εκτός ελέγχου και η μέση τιμή είναι στατιστικά άχρηστη. Αυστηρότερα προσδιορίζεται ως εξής. Λαμβάνουμε τις επαναλαμβανόμενες τιμές και προσαρμόζουμε ένα γραμμικό μοντέλο:

$$
Y=w_0N+w_1\\
$$

Τότε αν η κλίση είναι στατιστικά σημαντική $w_0\ne 0 \pm s_{w_0}$ τότε υπάρχει ολίσθηση. Μια απλή περιγραφή των σφαλμάτων είναι:

$$
x_i=\mu+ b+ e
$$
Για μία μέτρηση. Όπου $b$ είναι το συστηματικό και $e$ το τυχαίο σφάλμα. Αντίστοιχα, για πολλαπλές μετρήσεις:

$$
x_i=\mu+(\bar{x}-\mu)+(x_i-\mu)
$$

Μπορούμε να αποικονήσουμε τις τιμές ως μια *κατανομή* σε ένα *ιστόγραμμα*, δηλαδή μια απεικόνηση ευρών τιμών (bin) για συνεχείς μεταβλητές ή μεμονομένων τιμών για διακριτές μεταβλητές. Ο μέγιστος επιθυμητός αριθμός *κλάσσεων* ή εύρος ανά κλάσση δίνεται από τον *κανόνα Sturges*

>**Κανόνας Sturges:** $k = 1+3.322\log_{10}{N}$

Στη πράξη, ο αριθμός προσαρμώζεται πάνω ή κάτω ώστε να είναι ποιό στρογγυλά τα εύροι. Μια εναλλακτική παρουσίαση της κατανομής είναι μέσω της *συσσωρευτικής συνάρτησης κατανομής*. Σε αυτή τη περίπτωση σε κάθε κλάση προστίθενται οι παρατηρίσεις των προηγούμενων.

Αριθμητική περιγραφή των πληθυσμών είναι οι παράμετροι *θέσης* ,*διασποράς* και *συμμετρίας*.

## Παράμετροι Θέσης

Οι παράμετροι θέσης πληθυσμού είναι:

* Μέση τιμή $\bar x =\frac{\sum{x_i}}{N}\;\;\; \bar x \rightarrow \mu, n \rightarrow \infty $
* Διάμεση Τιμή $x_{ \frac {n+1}{2} } n:\;\;\;περιττός\;\;\; \frac {x_{ \frac{n}{2} } + x_{\frac {n}{2}} +1 }{2}$
* Ρυθμισμένη Μέση Τιμή
* Επικρατούσα Τιμή
  
## Παράμετροι Διασποράς

Οι βασικές παράμετροι διασποράς είναι:

* Εύρος: $R=x_n-x_1$
* Τυπική Απόκλιση: $s=\sqrt{\frac {\sum_{i=1}^N{(\bar {x}-x_1)^2 }}{N-1} }$, ισχύει ότι $s \rightarrow\sigma, n\rightarrow\infty$
* Σχετική Τυπική Απόκλιση: $s_t=^{s}/_{\bar{x} }$
* Εκατοστιαία Τυπική Απόκλιση: $\%RSD \equiv\%CV=\frac {s}{\bar {x}}*100\%$
* Διακύμανση: $\nu\equiv Var\;\;\; s^2, \sigma^2$
* Μέση Απόλυτη Απόκλιση: $amd=\frac {\sum_{i=1}^N{|{\bar{x}-x_i|}}}{N}$
* Τυπικό Σφάλμα: $SE=\sqrt{\frac {\sum_{i=1}^N{(x_i-\bar{x})}}{N(N-1)}}$
* Εύρος Μεταξύ Ποσοστημοριακών Σημείων
  
## Παράμετροι Συμμετρίας

Οι παράμετροι συμμετρίας είναι μόνο δύο και μετράνε ουσιαστικά το ίδιο πράγμα:

* Ασυμμετρία: $g_i=\frac{\sum_{i=1}^{N}{(x_1-\bar{x})^3}}{(N-1)(N-2)s^3}$
* Κύρτωση: $g_2=\frac 1n \sum_{i=1}^N[\frac {x_i-\bar{x}}{s}]^4-4$
  
## Κανονική Ή Gaussιανή Κατανομή

Όπως είδαμε μπορούμε να επικονήσουμε την *κατανομή* ενός πληθυσμού, με ένα ιστόγραμμα είτε χρησιμοποιώντας την συχνότητα είτε την cdf (*cummulative density function*). Θέλουμε από μια κατανομή που αντιπροσωπεύει ένα συγκεκριμένο πλυθησμό, να καταλήξουμε σε κάποιον "παγκόσμιο" τρόπο κατανομής.

>**Παρατήρηση:**\
>Υπάρχουν γενικά, διάφορες κατανομές, όπως Διωνυμική, Bernoulli κλπ. Συνήθως μια τέτοια κατανομή είναι μια γενική περιγραφή ενός *τύπου* κατανομής. Μια συγκεκριμένη μορφή της κάθε κατανομής προσδιορίζεται από (i) την γενική μορφή στην οποία συμμοφώνεται (πχ Διωνυμική,Gauss), και (ii) ένα σύνολο παραμέτρων που δίνουν την ακριβή μορφή. Αυτές οι παράμετροι διαφέρουν για κάθε τύπο κατανομής. Για την Guass οι δύο αυτές παράμετροι είναι η τυπική απόκλιση (*σ*) και το μέσο (*μ*). Η Διωνυμική έχει ένα παράγωντα που ονομάζεται (*p*). Έτσι θα συμβολίζουμε γενικά
>$$
>Χ(e_1,e_2,[\dots], e_n)
>$$
>και θα εννούμε μια κατανομή γενικού τύπου ($X$) με παραμέτρους $e_1,e_2,\dots, e_n$. Με αυτή τη σύμβαση, θα γράφουμε
>$$
N(\mu,\sigma)
>$$
>και θα εννοούμε κατανομή Gauss με μέση τιμή $\mu$ και τυπική απόκλιση $\sigma$.

Η κατανομή Gauss δίνεται από την συνάρτηση κατνανομής $f$ που δίνει τη συχνότητα συναρτήση της παραμέτρου $χ$ και έχει ως εξής:

$$
f(x) = \frac {1}{\sigma \sqrt{2\pi} }e^{=\frac {(x-\mu)^2}{2{\sigma}^2} }
$$

![κατανομή Gauss βασική]()

Μια άλλη ενδιαφέρουσα μορφή της συνάρτησης προκύπτει θέτοντας απλά $z=\frac {x-\mu}{\sigma}$. Τότε η συνάρτηση γίνεται:

$$
f(x) = \frac{1}{\sqrt{2\pi}}\int_{t=-z}^{t=+z}{e^{-t^2}dt}
$$
Αυτή η απεικόνηση δίνετ τη συχνότητα συναρτίση του αριθμού τυπικών αποκλίσεων. Οι τιμές του ολοκληρώματος μπορούν να προκύψουν από πίνακες ή με αριθμητικές μεθόδους (*βλ. SciPy.special.erf,  SciPY.stats.multivariate_normal,  SciPy.stats.norm*).

![μηχανη πληθυσμου Gauss]()

## Κατανομή Μέσω Τιμών

$$
\sigma_{\bar{x}} = \frac {\sigma_x}{\sqrt{N}}
$$

## Στατιστικές Δοκιμασίες Σημαντικότητας

Στις δοκιμασίες σημαντικότητας γνεικά, ξεκινάμε με μια αρχική υπόθεση $Η_0$ που ονομάζεται *μηδενική υπόθεση*, και εκτελούμε μια στατιστική δοκιμασία. Η δοκιμασία θα μας οδηγήσει στην αποδοχή της μηδενικής υπόθεσης ή την αποδοχή της, μέ κάποι δεδομένα όρια μέγιστης πιθανότητας λάθους (*στάθμη εμπιστοσύνης*). Όταν απορρίπτουμε την μηδενική υπόθεση, υιοθετούμε την *εναλλακτική υπόθεση* $H_a$. Η ταυτότητα των $H_0$ και $H_a$ εξαρτάται από την δοκιμασία. Γενικά ως μηδενική υπόθεση δεχόμασται οτι δεν υπάρχει σημαντική διαφορά και ως εναλλακτική οτι υπάρχει.
Μερικές κοινές εκδοχές είναι:

>$H_0$ οι διαφορές των δύο είναι τυχαίες, οπότε $H_a$ οι διαφορές δεν είναι τυχαίες\
>$H_0$ η τιμή ανοίκη στον πληθυσμό, $H_a$ η τιμή δεν ανοίκει στον πληθυσμό\
>$H_0:$ $\bar{x}= \mu$, $H_a: \bar{x}\ne\mu$\
>$H_0:\mu_A = \mu_B\;\; H_A: \mu_A\ne\mu_B$\
>$H_0:\sigma_A=\sigma_B,\;\;H_A:\sigma_A\ne\sigma_B$\
$H_0:X_i\in A_i\;\; H_a:X_i\notin A_i$\
>**Παρατήρηση:**
>Ισχύει πάντα:
>$$
>P(H_0)+P(H_a) = 1
>$$

Σφάλμα **πρώτου είδους** έχουμε όταν απορρίπτουμε την μηδενική ενώ ισχύει και σφάλμα **δεύτερου είδους** έχουμε όταν απορρίπτουμε την εναλλάκτική υπόθεση ενώ αυτή ισχύει. Η *στάθμη εμπιστοσύνης* εκφράζει την ελάχιστη πιθανότητα σφάλματος πρώτου είδους. Είναι συνήθως 95%, σπανιότερα 90% (όταν οι απαιτήσεις είναι χαμηλές) ή 95% (όταν οι απαιτήσεις είναι υψηλές).

![σφάλματα και στάθμες εμπιστοσύνης]()

Το αποτέλεσμα μια δοκιμασίας γενικά αποδίδεται μέσω της τιμής $P$ που είναι η πιθανότητα σφάλματος πρώτου είδους. Όταν αυτή είναι **μεγαλύτερη** από την τιμή 1-στάθμη εμπιστοσύνης τότε η μηδενική ισχύει. Έστων $CL$ ή στάθμη εμπιστοσύνης. Τότε:

$$
P>1-CL \implies H_0\\
P<1-CL \implies H_a
$$

Διότι η τιμή $1-CL$ είναι η πιθανότητα σφάλματος πρώτου είδους. Για παράδειγμα αν $P=0.03\;\; και\;\; CL=90\%$

## Δοκιμασία T ή Student's Test ή T-Test

